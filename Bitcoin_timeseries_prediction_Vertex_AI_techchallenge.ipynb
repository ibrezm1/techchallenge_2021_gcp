{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin-timeseries-prediction-Vertex-AI-techchallenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "environment": {
      "name": "common-cpu.m69",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m69"
    },
    "kernelspec": {
      "name": "python3613jvsc74a57bd01f096e30f60da1a9bf17c8055e3c834d6832f024e66271f2c127fba3f2712fca",
      "display_name": "Python 3.6.13 64-bit ('aiplatform-optimizer': pyenv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "# Vertex AI Bitcoin Model - Prediction\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The dataset we are using is the Bitcoin dataset from kaggle\n",
        "\n",
        "### Objective\n",
        "\n",
        "This notebook demonstrates, using the Vertex AI Python client library, how to train and make predictions on an AutoML timeseries model based on a timeseries dataset.\n",
        "\n",
        "The steps performed include the following:\n",
        "\n",
        "- Create a Vertex AI model training job.\n",
        "- Train an AutoML Timeseries forecasting model.\n",
        "- Deploy the `Model` resource to a serving `Endpoint` resource.\n",
        "- Make a prediction by sending data.\n",
        "- Undeploy the `Model` resource.\n",
        "\n",
        "### Costs \n",
        "\n",
        "This uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI :  ~14 $/hour of training, will require 1 hour for training the forecasting jobs\n",
        "* Cloud Storage : minimal\n",
        "* Bigquery : minimal ( Free Tier Mostly )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "USER_FLAG = \"\"\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqK0vAhGWWuD"
      },
      "source": [
        "Install the latest version of the Vertex AI client library.\n",
        "\n",
        "Run the following command in your virtual environment to install the Vertex SDK for Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl5jPE7uWWuD"
      },
      "source": [
        "!pip3 install {USER_FLAG} google-cloud-aiplatform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_storage"
      },
      "source": [
        "Install the Cloud Storage library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qssss-KSlugo"
      },
      "source": [
        "! pip3 install {USER_FLAG} google-cloud-storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHF-BJlyWWuF"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "This notebook demonstrates how to use Model Builder SDK to create an AutoML model based on a tabular dataset. You will need to provide a Cloud Storage bucket where the dataset will be stored.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all of your \n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"[your-region]\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf221059d072"
      },
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlhuP7dWWuN"
      },
      "source": [
        "### Copy dataset into your Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CYSg5agXWWuN"
      },
      "source": [
        "IMPORT_FILE = 'coin_Bitcoin.csv'\n",
        "PREDICT_FILE = 'coin_Bitcoin_predict.csv'\n",
        "! gsutil cp {IMPORT_FILE} {BUCKET_NAME}/data/\n",
        "! gsutil cp {PREDICT_FILE} {BUCKET_NAME}/data/\n",
        "\n",
        "gcs_source = f'{BUCKET_NAME}/data/{IMPORT_FILE}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Uo3tifg1kx"
      },
      "source": [
        "### Import Vertex SDK for Python\n",
        "\n",
        "Import the Vertex SDK into your Python environment and initialize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Wpc57hWWuO"
      },
      "source": [
        "### Create a Managed Tabular Dataset from a CSV\n",
        "\n",
        "This section will create a dataset from a CSV file stored on your GCS bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BpiiDIWWuP"
      },
      "source": [
        "ds = dataset = aiplatform.TimeSeriesDataset.create(\n",
        "    display_name='bitcoin-timeseries-dataset',\n",
        "    gcs_source = gcs_source,\n",
        ")\n",
        "\n",
        "ds.resource_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_aZmEPNWWuP"
      },
      "source": [
        "### Launch a Training Job to Create a Model\n",
        "\n",
        "Once we have defined your training script, we will create a model. The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2sEiUNlWWuP"
      },
      "source": [
        "job = aiplatform.AutoMLForecastingTrainingJob(\n",
        "    display_name='train-bitcoin-forecasting' ,\n",
        "    optimization_objective = 'minimize-mae'\n",
        ")\n",
        "\n",
        "# This will take around an hour to run\n",
        "model = job.run(\n",
        "    dataset=ds,\n",
        "    target_column='Open',\n",
        "    time_column = 'Date',\n",
        "    time_series_identifier_column = 'SNo',\n",
        "    unavailable_at_forecast_columns = ['High','Low','Open','Close','Volume','Marketcap'],\n",
        "    available_at_forecast_columns = ['date'],\n",
        "    forecast_horizon = 30,\n",
        "    context_window = 30,\n",
        "    data_granularity_unit = 'day',\n",
        "    data_granularity_count = 1,\n",
        "    export_evaluated_data_items = True,\n",
        "    budget_milli_node_hours = 100, # means 1 node hour - Cost will be ~14 dollars \n",
        "    model_display_name='bitcoin-forecast-model'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTrjJFJzWWuQ"
      },
      "source": [
        "### Create a prediction job for your model\n",
        "\n",
        "You use your model to make predictions, This function does two things:\n",
        "\n",
        "1. Creates an `Job` resource \n",
        "2. Runs the `Job` resource \n",
        "\n",
        "### NOTE: Wait until the model **FINISHES** deployment before proceeding to prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcUkGfyUWWuQ"
      },
      "source": [
        "predictJob = aiplatform.BatchPredictionJob (\n",
        "    batch_prediction_job_name = 'bitcoin-predection-job'\n",
        ")\n",
        "\n",
        "prediction = predictJob.create(\n",
        "    job_display_name = 'bitcoin-redict-job-run',\n",
        "    model_name = 'bitcoin-forecast-model',\n",
        "    instances_format = 'csv',\n",
        "    predictions_format ='bigquery',\n",
        "    gcs_source = f'{BUCKET_NAME}/data/{PREDICT_FILE}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACIdvX9eWWuQ"
      },
      "source": [
        "### Predict on the endpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkzeSNnTWWuR"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created :\n",
        "\n",
        "- Training Job\n",
        "- Model\n",
        "- Cloud Storage Bucket\n",
        "\n",
        "**Note**: You must delete any `Model` resources deployed to the `Endpoint` resource before deleting the `Endpoint` resource."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLo27kttWWuR"
      },
      "source": [
        "delete_training_job = True\n",
        "delete_model = True\n",
        "delete_endpoint = True\n",
        "\n",
        "# Warning: Setting this to true will delete everything in your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "# Delete the training job\n",
        "job.delete()\n",
        "\n",
        "# Delete the model\n",
        "model.delete()\n",
        "\n",
        "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}